#!/bin/bash

# This script is:
# Installing Spark (which is running in a local mode, on a single machine)
# Installing and launching Jupyter Notebook (which runs in a background)
# Sets up Kubernetes

# This script will be executed on a VM as a root user. It will be at first rendered using the Terraform templatefile function.
# We are using here variables provided by that function:
# - username
# - host_entries
# - ssh_private_key
# - jupyter_notebook_password


# In that script we have the following sections:
# - Sections 1-2 are assigning hostnames to the private IP addresses of both VMs and setting up a passwordless SSH connection 
#   from VM1 to VM2.
# - Sections 3-5 are setting up Spark (running in a local mode on VM1).
# - Section 6 is setting up a Jupyter Notebook.
# - Sections 7-13 are setting up Kubernetes.  


# === Section 1: Assign given hostnames to the private IP addresses of both VMs in the /etc/hosts file. ===

HOSTS_FILE="/etc/hosts"

echo "Adding entries to $HOSTS_FILE..."

# Entries to add to the /etc/hosts, that is lines mapping hostnames to IP addresses.
hosts_entries=( %{ for entry in host_entries ~} "${entry}" %{ endfor ~} )

for entry in "$${hosts_entries[@]}"; do
  # Check if entry already exists
  if grep -q "$entry" "$HOSTS_FILE"; then
    echo "Entry '$entry' already exists. Skipping."
  else
    # add entry to the hosts file
    echo "$entry" >> "$HOSTS_FILE"
    echo "Added: $entry"
  fi
done

echo "Done."



# === Section 2: Saving a SSH private key which will be used for connecting from the VM1 to the VM2 ===

mkdir -p /home/${username}/.ssh # Create the .ssh directory if it doesn't exists
chmod 700 /home/${username}/.ssh # Set correct permissions

# File to store the private key
KEY_FILE="/home/${username}/.ssh/id_rsa"

echo "Creating SSH key file..."
echo "${ssh_private_key}" >> "$KEY_FILE" # Save the SSH private key to the file

chmod 600 "$KEY_FILE" # Set correct permissions for that file.
chown -R ${username}:${username} /home/${username}/.ssh # Assign the ${username} user as the owner of the .ssh folder.

echo "SSH private key saved to $KEY_FILE with restricted permissions."







# # === Below sections are setting up Spark and Jupyter Notebook. ===

# # === Section 3: Install Java ===

apt-get update
apt-get install openjdk-8-jdk -y


# === Section 4: Download Spark and save it in the /home/${username}/spark. ===

apt-get install curl
curl -L https://archive.apache.org/dist/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz | tar -xz -C /home/${username}
mv /home/${username}/spark-3.4.4-bin-hadoop3 /home/${username}/spark


# === Section 5: Modify the .bashrc file. We will define there environment variables needed for Spark. ===

# add all the below lines to the .bashrc file
cat << 'EOF' >> /home/${username}/.bashrc
export SPARK_HOME="/home/${username}/spark"
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$SPARK_HOME/bin
EOF



# === Section 6: Set up Jupyter Notebook. ===

apt-get install python3.10 # Install python
DEBIAN_FRONTEND=noninteractive apt-get install python3-pip -y # Install pip
# add /home/${username}/.local/bin to the path so we can use the pip command (pip is located in that folder).
echo 'export PATH=$PATH:/home/${username}/.local/bin' >> /home/${username}/.bashrc
PATH=$PATH:/home/${username}/.local/bin

pip install notebook # install Jupyter Notebook
pip install pyspark==3.4.4 # here we need the same version as a version of Spark which we downloaded.
pip install findspark # install other needed libraries

# Set up a password to the Jupyter Notebook.

# generate the .jupyter/jupyter_notebook_config.py file. We need to run that command as the 'username' user in order to create that file
# in the /home/username folder.
sudo -u ${username} bash -c "jupyter notebook --generate-config" 

# Generate hashed password using the Jupyter 7+ module and the Terraform variable jupyter_notebook_password (default = 'admin').
HASHED_PASSWORD=$(python3 -c "from jupyter_server.auth import passwd; print(passwd('${jupyter_notebook_password}'))")

# Add the below lines to the jupyter_notebook_config.py file
cat << EOF >> /home/${username}/.jupyter/jupyter_notebook_config.py
c.NotebookApp.password = u'$HASHED_PASSWORD' # here we define our password to the Jupyter Notebook.
c.NotebookApp.open_browser = False
c.NotebookApp.ip = '0.0.0.0'
c.NotebookApp.port = 8888
EOF

# We need to define those variables in this shell session because here we are starting Jupyter Notebook.
SPARK_HOME=/home/${username}/spark
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# Start the Jupyter Notebook. The below command will start the Jupyter Notebook, run it in the background and it will
# continue running even after the current shell session ends. Breakdown of the command:
# 1) sudo -u ${username} bash -c "command" -> Run a command as the 'username' user. Recommended for security reasons.
# 2) cd /home/${username} -> change the directory before running the command for starting Jupyter. We want Jupyter to run in that folder
#    so we see that folder conent in the Jupyter website.
# 2) nohup -> It causes that the process started by the command will be running even after the shell session ends.
# 3) jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser -> Start the Jupyter Notebook.
# 4) > /home/${username}/jupyter.log 2>&1 -> Redirect output logs from the process to the /home/${username}/jupyter.log file.
# 5) & (at the end of the command) -> Run the process in the background.
sudo -u ${username} bash -c "cd /home/${username} && nohup jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser > /home/${username}/jupyter.log 2>&1 &"








# === Below sections are setting up Kubernetes. ===

# === Section 7: disable swap (it's required by Kubernetes). ===

swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab


# === Section 8: Install containerd. That is a container runtime used for running containers. ===

apt-get install -y containerd
mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml

# Change the 'SystemdCgroup = false' into the 'SystemdCgroup = true' in the /etc/containerd/config.toml
# This tells the containerd to use the systemd to manage resource (CPU, memory) just like the host system.
sed -i 's/^\([[:space:]]*SystemdCgroup = \)false/\1true/' /etc/containerd/config.toml
systemctl restart containerd

# Add this line to the /etc/crictl.yaml so the containerd knows that it should use the /var/run/containerd/containerd.sock socket.
# Otherwise after running 'crictl ps' we will see an error of something like '/var/run/cri-dockerd.sock: no such file or directory'.
echo "runtime-endpoint: unix:///var/run/containerd/containerd.sock" >> /etc/crictl.yaml


# === Section 9: Install kubeadm, kubelet, and kubectl ===

apt-get install -y apt-transport-https ca-certificates
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add the GPG key and APT repository URL to the kubernetes.list. That url will be used to pull Kubernetes packages (like kubectl)
cat << EOF >> /etc/apt/sources.list.d/kubernetes.list
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /
EOF

apt-get update
echo | apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl


# === Section 10: Initialize Kubernetes using Calico. ===

# load the br_netfilter module
modprobe br_netfilter
echo 'br_netfilter' | tee /etc/modules-load.d/k8s.conf

# Modify the sysctl
cat <<EOF | tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

# Apply the settings
sysctl --system

# Here we specify the CIDR, that is a IP address range from which Kubernetes will be assigning private IP addresses to pods.
kubeadm init --pod-network-cidr=192.168.0.0/16


# === Section 11: Set up kubectl for the user. It is a tool for users to interact with a Kubernetes cluster. ===

mkdir -p /home/${username}/.kube
cp -i /etc/kubernetes/admin.conf /home/${username}/.kube/config
chown ${username}:${username} /home/${username}/.kube/config


# === Section 12: Install Pod Network from Calico. ===

# We need to run that command as the 'username' user.
sudo -u ${username} kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml


# === Section 13: Get the join command for adding worker nodes. That command will need to be executed on every worker node. ===

kubeadm token create --print-join-command >> /home/${username}/k8s_join_workers.txt







# =========== Below sections are installing Docker ===========

# Add Docker's official GPG key:
apt-get update
apt-get -y install ca-certificates curl
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo \"$${UBUNTU_CODENAME:-$$VERSION_CODENAME}\") stable" | \
  tee /etc/apt/sources.list.d/docker.list > /dev/null
apt-get update

# install latest version of Docker. The '--no-install-recommends' option is needed to install Docker without the containerd.io.
# That's because we want to keep our existing containerd.
DEBIAN_FRONTEND=noninteractive apt-get -y install docker-ce docker-ce-cli docker-buildx-plugin docker-compose-plugin --no-install-recommends

# add user to the docker group in order to allow using docker commands on the created VM after connecting to it using SSH.
# It is not necessary to run docker containers automatically through Azure Pipelines Self Hosted Agents.
usermod -aG docker ${username}