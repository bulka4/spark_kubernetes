#!/bin/bash

# That script will be executed on a VM as a root user. It will be at first rendered using the Terraform templatefile function.
# Because of that we need to sometimes escape special characters, more info here: 
# https://developer.hashicorp.com/terraform/language/expressions/strings
# We are using here variables provided by that function:
# - username
# - jupyter_notebook_password
# - acr_url
# - acr_sp_id
# - acr_sp_password



# ========= Install containerd. That is a container runtime used for running containers in Kubernetes. =========

apt-get install -y containerd
mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml

# Change the 'SystemdCgroup = false' into the 'SystemdCgroup = true' in the /etc/containerd/config.toml
# This tells containerd to use the systemd to manage cgroups which manages resources (CPU, RAM etc).
# I think it is necessary because kubelet is using systemd for that purpose as well.
sed -i 's/^\([[:space:]]*SystemdCgroup = \)false/\1true/' /etc/containerd/config.toml
systemctl restart containerd

# Add a path to the containerd socket to the crictl.yaml so the crictl can communicate with containerd.
# It is not needed for running Kubernetes but it might be useful for debugging containerd using crictl.
echo "runtime-endpoint: unix:///var/run/containerd/containerd.sock" >> /etc/crictl.yaml






# ======= Install kubeadm, kubelet, and kubectl - Tools needed for Kubernetes. ========

apt-get install -y apt-transport-https ca-certificates
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add the GPG key and APT repository URL to the kubernetes.list. That url will be used to pull Kubernetes packages (like kubectl)
cat << EOF >> /etc/apt/sources.list.d/kubernetes.list
deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /
EOF

apt-get update
echo | apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl







# ========= Initialize Kubernetes using Calico. ==========

# disable swap (it's required by Kubernetes).
swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab

# create a k8s folder. We will be using it for storing files relevant to Kubernetes (like manifests).
mkdir /home/${username}/k8s

# Load the br_netfilter module. It is needed so the iptables can filter network traffic that goes through Linux bridges,
# such as communication between Kubernetes Pods.
modprobe br_netfilter
echo 'br_netfilter' | tee /etc/modules-load.d/k8s.conf

# Modify kernel parametrs using sysctl
cat <<EOF | tee /etc/sysctl.d/k8s.conf
# Enable filterring network traffic that goes through Linux bridges using iptables
net.bridge.bridge-nf-call-iptables = 1
# Enable IP forwarding for IPv4
net.ipv4.ip_forward = 1
EOF

# Apply the settings
sysctl --system

# Here we specify the CIDR, that is an IP address range from which Kubernetes will be assigning private IP addresses to pods.
# Value of the pod-network-cidr must match the CNI plugin's (Calico's in this case) expectiations.
kubeadm init --pod-network-cidr=192.168.0.0/16

# The kubernetes cofnig file (admin.conf) is used to connect and authenticate to Kubernetes. When a Linux user uses the kubectl 
# command to interact with Kubernetes, that configuration file will specify what Kubernetes user will be used to perform actions
# from that kubectl command (like create resources).
# When we run kubectl commands then it will use the config file from the path specified by the KUBECONFIG env variable or, if it's
# not specified, from the default location at ~/.kube/config.

# Specify KUBECONFIG env variable which will be used when we run kubectl commands later on in this script as root. It will give us
# proper permissions in Kubernetes cluster. Otherwise kubectl commands might not work (and fail silently).
export KUBECONFIG=/etc/kubernetes/admin.conf

# Set up the /home/${username}/.kube/config with Kubernetes config file which will be used by the ${username} Linux user later on
# once we connect to that VM through SSH.
mkdir -p /home/${username}/.kube
cp -i /etc/kubernetes/admin.conf /home/${username}/.kube/config
chown ${username}:${username} /home/${username}/.kube/config

# Deploy Calico (a CNI used for networking). We need to run that command as the 'username' user.
sudo -u ${username} kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml


# Get the join command for adding worker nodes. That command will need to be executed on every worker node in order to join them
# to the cluster.
kubeadm token create --print-join-command >> /home/${username}/k8s_join_workers.txt








# ======== Wait for the Kubernetes to be ready ============

# Before we progress with the script we need to wait until Kubernetes is ready so we can use the kubectl command.

echo "Waiting for Kubernetes to be ready..."
for i in {1..60}; do
    if kubectl version >/dev/null 2>&1; then
        echo "Kubernetes is ready."
        break
    fi
    echo "Waiting for Kubernetes... ($i)"
    sleep 5
done

# If Kubernetes is not ready then stop the script and save logs from executing the kubectl command.
if ! kubectl version >/dev/null 2>&1; then
    echo "Kubernetes API never became ready after waiting. Exiting."
    # Save the logs from executing kubectl command
    kubectl version | tee k8s_version_check.log
    exit 1
fi










# ========= Create a Dockerfile for setting up Spark and Jupyter Notebook. ============

# That Dockerfile builds an image which can launch a Jupyter Notebook or not depending on the value of the LAUNCH_JUPYTER
# environmental variable. If LAUNCH_JUPYTER = true then it will launch Jupyter, otherwise it will not launch it.
# That logic is implemented in the entrypoint.sh file which we create later on in this script and which is used in the Dockerfile
# (it is executed when we run a container).

# We want to launch Jupyter in order to run Spark code in a local mode to develop and test Spark code. Once the Spark script is
# prepared, we can use the same Docker image in order to submit a Spark job on Kubernetes.

# SPARK_USERNAME is a username for the spark user which we will create in the Docker container. That user will be running the container
# (Spark and Jupyter Notebook). We will be running Jupyter in a specific folder so the content of that folder will be visible in Jupyter
# and there we will be creating new files from Jupyter. When we create files from Jupyter then the spark user running it will be 
# verified for permissions to do so, so that user needs to have a write permission to the folder where we run Jupyter Notebook.

# Folder where we will be running Jupyter will be mounted to a folder on host so the spark user needs permissions to the mounted
# folder on host as well. For defining those permissions we need to use UID and GID of that user. That's why we will create that user in the
# Docker container with a specific UID and GID so we know what are those values. Otherwise those values would be generated automatically
# and we wouldn't know their value.
SPARK_USERNAME=spark
SPARK_UID=999
SPARK_GID=999


mkdir /home/${username}/docker
# Create an entrypoint script which will be used in the Dockerfile. If env variable LAUNCH_JUPYTER=true, then it will run Jupyter
# in the specified folder, listening on the specified port. Otherwise it will start a bash session so the container doesn't exit
# immediately.
cat << EOF >> /home/${username}/docker/entrypoint.sh
#!/bin/bash

if [ "\$LAUNCH_JUPYTER" = "true" ]; then
    echo "Starting Jupyter Notebook..."
    exec jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --notebook-dir=/home/$SPARK_USERNAME/notebooks
else
    echo "Running Spark job..."
    # start a bash shell which we can use after starting container, so the container doesn't exit immediately.
    exec bash
fi
EOF



# Create the Dockerfile
cat << EOT >> /home/${username}/docker/spark_jupyter.dockerfile
# Use an official OpenJDK base image
FROM openjdk:8-jdk


# ====== Section 1: Set up Spark to run in a local mode. ======

# Define environment variables needed for Spark.
ENV SPARK_VERSION=3.4.4 \\
    # username of the spark user which will be running Spark and Jupyter Notebook.
    SPARK_USERNAME=$SPARK_USERNAME \\
    # Directory where we will install Spark.
    SPARK_HOME="/opt/$SPARK_USERNAME" \\
    # This prevents from prompting user for input when installing tools with apt-get
    DEBIAN_FRONTEND=noninteractive
    
ENV PATH=\$${PATH}:\$${SPARK_HOME}/bin

# This instruction causes that bash will be used for all the further commands specified in the 'RUN' instructions.
SHELL ["/bin/bash", "-c"]

# Create the spark user and group with specific UID and GID.
RUN groupadd -r -g $SPARK_GID \$${SPARK_USERNAME} && useradd -r -u $SPARK_UID -g \$${SPARK_USERNAME} -m -d /home/\$${SPARK_USERNAME} \$${SPARK_USERNAME}

# Download Spark and save it in the \$SPARK_HOME.
RUN apt-get update && \\
    apt-get install curl -y && \\
    curl -L https://archive.apache.org/dist/spark/spark-\$${SPARK_VERSION}/spark-\$${SPARK_VERSION}-bin-hadoop3.tgz | tar -xz -C /opt && \\
    mv /opt/spark-\$${SPARK_VERSION}-bin-hadoop3 \$${SPARK_HOME} && \\
    # We will be running a Jupyter Notebook as the spark user, so in order to use Spark in that Jupyter Notebook, the spark user needs to have
    # the read permission to the folder where Spark is installed.
    # Below command grants the following permissions: root - full access, others - read only.
    chmod 755 \$${SPARK_HOME}






# ====== Section 2: Set up Jupyter Notebook. ======

# Install Python, pip, Jupyter Notebook and PySpark
RUN \\
    # Install Python
    apt-get install python3.10 -y && \\
    # Install pip
    apt-get install python3-pip -y && \\
    # Install Jupyter Notebook and PySpark
    pip install notebook pyspark==\$${SPARK_VERSION} findspark


COPY entrypoint.sh /opt/entrypoint.sh

# make sure the entrypoint.sh script is executable.
RUN chmod +x /opt/entrypoint.sh


# Set up a password to the Jupyter Notebook provided by the Terraform variable jupyter_notebook_password (default = 'admin'). 
# We need to run all the below commands as the spark user in order to:
# - create the jupyter_notebook_config.py file in the spark user home folder - /home/\$SPARK_USERNAME.
# - Make the spark user an owner of the /home/\$${SPARK_USERNAME}/notebooks. That's where Jupyter will be running.
# - Run Jupyter Notebook as that user (that is run the entrypoint.sh script).
USER \$${SPARK_USERNAME}
RUN \\
    # generate the .jupyter/jupyter_notebook_config.py file. We need to run that command as the spark user in order to create that file
    # in the /home/\$SPARK_USERNAME folder. In that file we will define a password.
    jupyter notebook --generate-config && \\

    # Generate hashed password using the Jupyter 7+ module and the Terraform variable jupyter_notebook_password.
    HASHED_PASSWORD=\$(python3 -c "from jupyter_server.auth import passwd; print(passwd('${jupyter_notebook_password}'))") && \\

    # Add the below lines to the jupyter_notebook_config.py file
    echo -e "c.NotebookApp.password = u'\$HASHED_PASSWORD' # here we define our password to the Jupyter Notebook.\n\\
c.NotebookApp.open_browser = False\n\\
c.NotebookApp.ip = '0.0.0.0'\n\\
c.NotebookApp.port = 8888" \\
>> /home/\$${SPARK_USERNAME}/.jupyter/jupyter_notebook_config.py && \\

    # Create a folder where Jupyter Notebook will run. Content of that folder will be visible on the Jupyter website. 
    mkdir /home/\$${SPARK_USERNAME}/notebooks


# Expose the default Jupyter Notebook port
EXPOSE 8888

ENTRYPOINT ["/opt/entrypoint.sh"]
EOT

# Make the 'username' user an owner of the docker folder, so it can also use those files.
chown -R ${username}:${username} /home/${username}/docker







# =========== Install Docker ===========

# This installation of Docker is slightly different than the standard one since we already installed containerd when
# setting up Kubernetes.

# This prevents prompting user for input for example when using apt-get.
DEBIAN_FRONTEND=noninteractive

# Add Docker's official GPG key:
apt-get update
apt-get -y install ca-certificates curl
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo \"$${UBUNTU_CODENAME:-$$VERSION_CODENAME}\") stable" | \
  tee /etc/apt/sources.list.d/docker.list > /dev/null
apt-get update

# install latest version of Docker. The '--no-install-recommends' option is needed to install Docker without the containerd.io.
# That's because we want to keep our existing containerd used for Kubernetes.
apt-get -y install docker-ce docker-ce-cli docker-buildx-plugin docker-compose-plugin --no-install-recommends

# We need to have containerd running for using Docker. It might be stopped after we installed it for Kubernetes.
systemctl unmask containerd
systemctl enable containerd
systemctl start containerd

# create a docker group if it doesn't exist yet
getent group docker || groupadd docker
# add user to the docker group in order to allow it using docker commands
usermod -aG docker ${username}

# Restart Docker
systemctl unmask docker.socket
systemctl enable docker.socket
systemctl restart docker.socket







# ======= Build a Docker image and push it to the ACR ==============

# We are pushing the Docker image to ACR so it can be used by the SparkApplication Kubernetes resource. We are specifying its URL
# in the SparkApplication YAML manifest.

IMAGE_NAME=spark-jupyter
ACR_IMAGE="${acr_url}/$${IMAGE_NAME}:latest" # ACR URL of the format myregistry.azurecr.io

# Build Docker image
docker build -f /home/${username}/docker/spark_jupyter.dockerfile -t $IMAGE_NAME /home/${username}/docker
# Login to the ACR
docker login ${acr_url} --username ${acr_sp_id} --password ${acr_sp_password}

# Tag the local image for ACR
docker tag $IMAGE_NAME $ACR_IMAGE

# Push the image to ACR
docker push $ACR_IMAGE







# ========= Run Docker container with Spark and Jupyter Notebook in the background ============

# That container will be mounted to the /home/${username}/notebooks folder in order to save there scripts developed in Jupyter.

mkdir /home/${username}/notebooks

# Create a sample Spark script for running in Kubernetes (for testing).
cat << 'EOF' >> /home/${username}/notebooks/my_script.py
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("SparkTest") \
    .getOrCreate()

df = spark.range(0, 10)
df.show()
EOF

# Make the spark user an owner of that folder. The spark user is the one which we will be running a container (including the Spark
# script and Jupyter Notebook inside) so it needs to have access to this folder containing Spark script to run.
chown $SPARK_UID:$SPARK_GID -R /home/${username}/notebooks


# Run a Docker container with the following options:
# - LAUNCH_JUPYTER=true inidcates that we want to start a Jupyter Notebook.
# - mount the /home/${username}/notebooks folder from VM to the /home/$SPARK_USERNAME/notebooks folder in a container so the 
#   files which we create in Jupyter Notebook get saved on the host.
# - --rm indicates that the container will be removed once it exits.

docker run -d -p 8888:8888 \
  -e LAUNCH_JUPYTER=true \
  --mount type=bind,source=/home/${username}/notebooks,target=/home/$SPARK_USERNAME/notebooks \
  --rm --name $IMAGE_NAME $IMAGE_NAME








# ======== Install Helm, use it to install Spark Operator and assign permissions to Spark Operator. ========

# Helm is a package manager for Kubernetes which we will use to install Spark Operator.
# Spark Operator is needed for submitting Spark jobs to Kubernetes. It creates a Spark Driver Pod when we deploy
# the SparkApplication resource, and Driver Pod creates Spark Executors Pods later on. That Spark Operator will 
# automatically get assigned a Service Account with a proper permissions by Helm.

# We need to have webhook enabled so we can set up tolerations in the SparkApplication manifest.


# Install Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Install Spark Operator using Helm. 
helm repo add spark-operator https://kubeflow.github.io/spark-operator
helm repo update

NAMESPACE="spark-operator"

# --set sparkOperatorVersion=v1beta2-1.3.8-3.1.1 \
helm install spark-operator spark-operator/spark-operator --namespace $NAMESPACE --create-namespace \
  --set sparkJobNamespace=default \
  --set sparkOperatorVersion=v1beta2-1.3.9-3.4.0 \
  --set webhook.enable=true \
  --set webhook.enableCertManager=false \
  --set webhook.generateSelfSignedCert=true







# ========= Patch the Spark Operator Controller and Webhook deployments ===========

# We need to patch Spark Operator deployments, for both controller and webhook, such that we add to them tolerations which will 
# allow for deploying them on the master node (control plane). This is needed because in this script we are starting spark operator
# deployments (controller and webhook) and we don't have any worker nodes added to the cluster yet, so those deployments will be
# created on the master node.

# Wait until spark operator pods are started
LABEL="app.kubernetes.io/name=spark-operator"
INTERVAL=5


for i in {1..12}; do
  # Check how many spark operator pods there is
  no_pods=$(kubectl -n $NAMESPACE get pods -l $LABEL --no-headers 2>/dev/null | wc -l)
  
  # If there is at least one spark operator pod then break the loop
  if [ "$no_pods" -gt 0 ]; then
    echo "Spark Operator pods are started!"
    break
  fi

  echo "Pods not running yet. Sleeping $INTERVAL seconds..."
  sleep $INTERVAL
done

# commands for debugging
echo "Below are the Pods running in the spark-operator namespace: "
kubectl -n $NAMESPACE get pods

echo "Below are the deployments in the spark-operator namespace: "
kubectl -n $NAMESPACE get deployments


# Patch Spark Operator Controller deployment
kubectl -n $NAMESPACE patch deployment spark-operator-controller --type merge -p '
spec:
  template:
    spec:
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
'
kubectl -n $NAMESPACE rollout status deployment spark-operator-controller


# Patch Spark Operator Webhook deployment
kubectl -n $NAMESPACE patch deployment spark-operator-webhook --type merge -p '
spec:
  template:
    spec:
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
'
kubectl -n $NAMESPACE rollout status deployment spark-operator-webhook








# ========== Deploy Kubernetes Service Account with proper permissions. ==========

# That Service Account will be used by the Spark Driver Pod for authentication when creating Spark Executors Pods.

# Create a manifest for deploying Service Account
cat << EOF >> /home/${username}/k8s/spark_service_account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: spark-role
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log", "services", "configmaps", "secrets"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]
- apiGroups: ["apps"]
  resources: ["statefulsets"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]
- apiGroups: ["sparkoperator.k8s.io"]
  resources: ["sparkapplications", "scheduledsparkapplications"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: spark-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: spark-role
subjects:
- kind: ServiceAccount
  name: spark
  namespace: default
EOF

# Deploy the Service Account.
kubectl apply -f /home/${username}/k8s/spark_service_account.yaml








# ====== Create a secret for authentication to ACR. ======

# Spark Driver and Executor Pods created by the SparkApplication resource will need to pull Docker image with Spark from ACR. In order to do 
# that we need to create a secret which will be used for authentication and use the imagePullSecrets parameter in the Spark Application manifest
# to get that secret.

# Here we create a new secret which holds a new value generated based on credentials to the ACR.
# Here it doesn't matter what email we provide as the docker-email parameter.
kubectl create secret docker-registry acr-auth \
  --docker-server=${acr_url} \
  --docker-username=${acr_sp_id} \
  --docker-password=${acr_sp_password} \
  --docker-email=unused@example.com \
  --namespace=default







# ====== Section 17: Prepare a SparkApplication manifest ======

# That SparkApplication manifest will be used for submitting Spark jobs. After deploying it, it will be used by
# the Spark Operator as described earlier in the section where we were installing Spark Operator with Helm.

# It creates a volume at the /home/${username}/notebooks path at host and mount it to the created Pods.
# That volume will contain Spark scripts to run.

# This SparkApplication resource runs the specified Spark script: my_script.py

# We are using here tolerations to allow deploying Spark Driver and Executors Pods on the control plane node (master node).
# Using tolerations requires enabling webhook in Spark Operator.

# Also we are using here imagePullSecrets to get a secret created before for authentication to the ACR when
# pulling a Docker image with Spark.


cat << EOF >> /home/${username}/k8s/spark_application.yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: pyspark-app
  namespace: $NAMESPACE
spec:
  type: Python
  mode: cluster
  pythonVersion: "3"
  imagePullPolicy: Always
  sparkVersion: "3.4.4"
  restartPolicy:
    type: Never

  # Spark script to run. That is a path inside the Spark Driver Pod's container.
  mainApplicationFile: "local:///opt/spark/scripts/my_script.py"

  # Docker image with Spark (a URL to the ACR from where we will pull an image)
  image: $ACR_IMAGE
  # Get a secret with value for authentication to the ACR.
  imagePullSecrets:
  - acr-auth
  
  # Create a volume with Spark scripts
  volumes:
  - name: scripts-volume
    hostPath:
      path: /home/${username}/notebooks   # path on the Kubernetes node (VM) with Spark scripts
      type: Directory


  # Specify parameters for the Spark Driver Pod
  driver:
    cores: 1
    memory: "512m"
    serviceAccount: spark # Use a proper service account created earlier for authentication for Spark Driver.
    labels:
      version: 3.3.1
    
    # Mount a volume with Spark scripts
    volumeMounts:
    - name: scripts-volume
      mountPath: /opt/spark/scripts  # path inside the Spark Driver Pod's container with Spark scripts

    # This toleration will allow us to deploy SparkApplication on the control plane node (master node)
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"


  # Specify parameters for the Spark Executors' Pods 
  executor:
    cores: 1
    instances: 2
    memory: "512m"
    labels:
      version: 3.3.1

    # Mount a volume with Spark scripts
    volumeMounts:
    - name: scripts-volume
      mountPath: /opt/spark/scripts  # path inside the container where Spark scripts will be accessible

    # This toleration will allow us to deploy SparkApplication on the control plane node (master node)
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
EOF


# The 'username' user need to have read and write permissions to that folder in order to deploy manifests
# saved there.
chown -R ${username}:${username} /home/${username}/k8s